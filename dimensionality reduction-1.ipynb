{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9484ecde-9e52-400c-928d-6622ce9556a4",
   "metadata": {},
   "source": [
    "# Question.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ebbc51-a311-4f63-9251-ebf250d31ad6",
   "metadata": {},
   "source": [
    "## What is the curse of dimensionality reduction and why is it important in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b72f0-21cb-4332-8ffd-a48adca19648",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" refers to the challenges and issues that arise when working with high-dimensional data in machine learning and data analysis. It's a term used to describe the negative consequences of having a large number of features or dimensions in a dataset. This phenomenon has several important implications:\n",
    "\n",
    "**1. Increased Computational Complexity:** As the number of dimensions increases, the computational resources (e.g., time and memory) required for processing and analyzing the data grow exponentially. Algorithms become slower and may even become infeasible to run with very high-dimensional data.\n",
    "\n",
    "**2. Data Sparsity:** In high-dimensional spaces, data points tend to become more sparse. This means that the available data becomes spread out, and it becomes difficult to find meaningful patterns or relationships between data points. The density of data points decreases as the dimensionality increases.\n",
    "\n",
    "**3. Overfitting:** High-dimensional data can lead to overfitting. With many features, a model can fit the noise in the data rather than capturing the underlying patterns. This can result in poor generalization to new, unseen data.\n",
    "\n",
    "**4. Increased Sample Size Requirements:** To maintain the same level of statistical significance when dealing with high-dimensional data, you often need a much larger sample size. As the dimensionality increases, the number of samples required to maintain statistical power grows exponentially.\n",
    "\n",
    "**5. Loss of Intuition and Visualization:** As the number of dimensions increases, it becomes challenging to visualize and interpret the data. Human intuition and traditional visualization techniques are limited to three dimensions, making it difficult to gain insights from high-dimensional data.\n",
    "\n",
    "**6. Curse of Dimensionality in Distance Metrics:** Many machine learning algorithms rely on distance metrics (e.g., Euclidean distance) to measure similarity or dissimilarity between data points. In high-dimensional spaces, all data points tend to be equally distant from each other, reducing the discriminatory power of these metrics.\n",
    "\n",
    "**Why Dimensionality Reduction is Important:**\n",
    "\n",
    "Dimensionality reduction techniques are essential in addressing the curse of dimensionality and mitigating its negative effects. These techniques aim to reduce the number of features while preserving the most relevant information. They are important for several reasons:\n",
    "\n",
    "1. **Improved Computational Efficiency:** Reducing the number of features makes algorithms more efficient, enabling faster training and inference.\n",
    "\n",
    "2. **Enhanced Generalization:** Dimensionality reduction helps reduce overfitting, leading to models that generalize better to new data.\n",
    "\n",
    "3. **Simpler Models:** Lower-dimensional data is easier to work with and interpret. It simplifies model building and facilitates human understanding of the data.\n",
    "\n",
    "4. **Visualization:** Reduced-dimensional data is often easier to visualize, allowing for better exploration and understanding of the data's structure.\n",
    "\n",
    "Common dimensionality reduction techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders. These methods transform high-dimensional data into a lower-dimensional representation while retaining as much meaningful information as possible. The choice of technique depends on the specific problem and goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53af75-aa27-4afa-9350-a857e238e90b",
   "metadata": {},
   "source": [
    "# Question.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca24c9-521d-4d29-a167-b14d36cdf1b3",
   "metadata": {},
   "source": [
    "##  How does the curse of dimensionality impact the performance of machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818fb903-07ef-4d90-8e17-3eb765e20dd5",
   "metadata": {},
   "source": [
    "The curse of dimensionality has a significant impact on the performance of machine learning algorithms in several ways:\n",
    "\n",
    "1. **Increased Computational Complexity**: As the dimensionality of the feature space increases, the computational complexity of many algorithms grows exponentially. This means that algorithms require more time and memory to process high-dimensional data. Training and inference become slower, which can be impractical for large datasets.\n",
    "\n",
    "2. **Overfitting**: High-dimensional data increases the risk of overfitting. With more features, machine learning models have a higher capacity to fit noise in the data rather than capturing the underlying patterns. Overfit models perform well on the training data but generalize poorly to unseen data.\n",
    "\n",
    "3. **Data Sparsity**: In high-dimensional spaces, data points tend to become more sparse. The number of data points required to maintain the same level of statistical significance grows exponentially with dimensionality. Sparse data makes it challenging to find meaningful patterns, and models may struggle to make accurate predictions.\n",
    "\n",
    "4. **Loss of Discriminatory Power in Distance Metrics**: Many machine learning algorithms rely on distance metrics (e.g., Euclidean distance) to measure similarity between data points. In high-dimensional spaces, the distances between data points tend to become more uniform, reducing the discriminatory power of these metrics. This can affect the performance of clustering, nearest neighbor search, and distance-based algorithms.\n",
    "\n",
    "5. **Increased Sensitivity to Noise**: High-dimensional data is often noisy, and noise can have a more significant impact on the model's performance in high-dimensional spaces. Models may capture noise as if it were a meaningful signal, leading to suboptimal results.\n",
    "\n",
    "6. **Higher Sample Size Requirements**: To maintain the same level of statistical power and significance, you need a much larger sample size when working with high-dimensional data. Collecting a sufficiently large dataset can be expensive and time-consuming.\n",
    "\n",
    "7. **Model Complexity**: High-dimensional models tend to be more complex, with a large number of parameters. This complexity can make models harder to interpret and explain, leading to reduced model transparency and interpretability.\n",
    "\n",
    "8. **Curse of Dimensionality in Feature Selection**: Feature selection, a process aimed at identifying the most relevant features, becomes more challenging in high-dimensional data. Identifying a subset of informative features is a complex optimization problem.\n",
    "\n",
    "To address these challenges, practitioners often employ dimensionality reduction techniques to reduce the number of features while preserving as much relevant information as possible. Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and feature selection methods are examples of techniques used to combat the curse of dimensionality. Careful feature engineering, regularization, and model selection are also essential for improving the performance of machine learning algorithms when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec458e-5d50-454a-bcc4-4403b0be08ec",
   "metadata": {},
   "source": [
    "# Question.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996edd5-b84f-4441-97b2-51debba4fdd1",
   "metadata": {},
   "source": [
    "## What are some of the consequences of the curse of dimensionality in machine learning, and how do they impact model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320ef63-f082-4507-a2cf-4c38670084ba",
   "metadata": {},
   "source": [
    "The curse of dimensionality in machine learning leads to several consequences that can significantly impact model performance:\n",
    "\n",
    "1. **Increased Computational Complexity**: As the number of features or dimensions increases, the computational resources required for training and inference grow exponentially. Algorithms become slower, making it impractical to work with high-dimensional data in real-time or with limited computational resources. This complexity can lead to long training times and increased hardware requirements.\n",
    "\n",
    "2. **Data Sparsity**: High-dimensional spaces tend to be sparser. This means that the data points become more spread out, and the density of data points decreases as dimensionality increases. Sparse data makes it challenging to find meaningful patterns or relationships between data points. As a result, models may struggle to make accurate predictions, and they may require larger datasets to maintain statistical significance.\n",
    "\n",
    "3. **Overfitting**: High-dimensional data increases the risk of overfitting. Models with many features have a higher capacity to fit noise in the data rather than capturing the true underlying patterns. This can lead to models that perform well on the training data but generalize poorly to new, unseen data. Overfitting can result in poor model performance and reduced predictive power.\n",
    "\n",
    "4. **Loss of Discriminatory Power in Distance Metrics**: Many machine learning algorithms rely on distance metrics (e.g., Euclidean distance) to measure similarity or dissimilarity between data points. In high-dimensional spaces, the distances between data points tend to become more uniform, reducing the discriminatory power of these metrics. This can affect the performance of clustering, nearest neighbor search, and distance-based algorithms, leading to suboptimal results.\n",
    "\n",
    "5. **Increased Sample Size Requirements**: To maintain the same level of statistical significance, you need a much larger sample size when dealing with high-dimensional data. Collecting a sufficiently large dataset can be challenging and expensive. Inadequate sample sizes can result in unreliable models with poor generalization performance.\n",
    "\n",
    "6. **Dimensionality Reduction**: To combat the curse of dimensionality, dimensionality reduction techniques such as Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are often employed. While these techniques can help reduce dimensionality and mitigate some of the issues, they can also result in loss of information and may not always capture the most relevant features.\n",
    "\n",
    "7. **Increased Sensitivity to Noise**: High-dimensional data is often noisy, and noise can have a more significant impact on model performance in high-dimensional spaces. Models may capture noise as if it were meaningful signal, leading to suboptimal results.\n",
    "\n",
    "8. **Model Complexity**: High-dimensional models tend to be more complex, with a large number of parameters. This complexity can make models harder to interpret and explain, leading to reduced model transparency and interpretability.\n",
    "\n",
    "To address these consequences, practitioners often need to carefully preprocess and feature-engineer the data, apply dimensionality reduction techniques, use regularization methods, and choose appropriate algorithms that are less susceptible to the curse of dimensionality. Proper feature selection, domain knowledge, and data understanding are also crucial for overcoming these challenges and achieving good model performance in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb769772-cf14-4e41-944e-0d4009e9504d",
   "metadata": {},
   "source": [
    "# Question.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d972f-02de-496c-a92f-1ef954023b45",
   "metadata": {},
   "source": [
    "## Can you explain the concept of feature selection and how it can help with dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd76f60f-5aea-4e60-8048-21e8fba527bb",
   "metadata": {},
   "source": [
    "**Feature selection** is a process in machine learning and data analysis where you choose a subset of the most relevant features (variables or columns) from your dataset while excluding the less important ones. The goal is to reduce the dimensionality of the data while retaining the most valuable information. Feature selection helps address the curse of dimensionality and offers several benefits:\n",
    "\n",
    "1. **Improved Model Performance**: By removing irrelevant or redundant features, feature selection can lead to simpler, more interpretable models that perform better on both training and validation data. Reducing noise in the data can enhance a model's predictive power.\n",
    "\n",
    "2. **Faster Training and Inference**: With fewer features, machine learning algorithms require less computational time and memory resources. This leads to faster model training and quicker predictions, which can be crucial for real-time applications.\n",
    "\n",
    "3. **Enhanced Model Generalization**: A reduced feature set can result in models that generalize better to unseen data. Overfitting is less likely when the model focuses on the most informative features, rather than fitting the noise.\n",
    "\n",
    "4. **Improved Interpretability**: Simpler models with fewer features are easier to interpret and explain to stakeholders or end-users. Understanding the key factors driving the model's predictions is essential in various applications.\n",
    "\n",
    "5. **Reduced Data Collection Efforts**: In situations where data collection is costly or time-consuming, feature selection can help reduce the amount of data needed. By focusing on the most relevant features, you can make the most of available data.\n",
    "\n",
    "There are several methods for feature selection:\n",
    "\n",
    "1. **Filter Methods**: These methods evaluate the relevance of features independently of the learning algorithm. Common techniques include chi-squared test, mutual information, and correlation analysis. Features are ranked or scored based on their individual importance.\n",
    "\n",
    "2. **Wrapper Methods**: Wrapper methods use a machine learning model's performance as the criterion for selecting features. Common approaches include forward selection, backward elimination, and recursive feature elimination (RFE). These methods involve training and evaluating the model with different subsets of features.\n",
    "\n",
    "3. **Embedded Methods**: Embedded methods incorporate feature selection into the model training process. For example, some tree-based algorithms like Random Forests and gradient boosting automatically provide feature importance scores, which can guide feature selection. L1 regularization (Lasso) is another example where the model itself selects a subset of features.\n",
    "\n",
    "4. **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) can be considered feature selection methods because they transform the data into a lower-dimensional space by creating new features that are linear or nonlinear combinations of the original features. These techniques focus on capturing the most relevant information in fewer dimensions.\n",
    "\n",
    "The choice of feature selection method depends on the nature of the data, the problem you're trying to solve, and computational resources. In practice, a combination of methods may be used to find the most informative feature set. Feature selection is an essential step in the data preprocessing pipeline and should be performed carefully to avoid information loss while reducing dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38168cc9-d8ef-4d46-8abc-42503c696e6e",
   "metadata": {},
   "source": [
    "# Question.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c8bad-72dd-4277-a879-5d71d71a9461",
   "metadata": {},
   "source": [
    "## What are some limitations and drawbacks of using dimensionality reduction techniques in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05872c0a-df26-4fbd-9d3f-710ea69d1499",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques are valuable tools in machine learning for addressing the curse of dimensionality and improving the performance of models. However, they come with limitations and drawbacks that practitioners should be aware of:\n",
    "\n",
    "1. **Loss of Information**: One of the primary drawbacks of dimensionality reduction is the potential loss of information. When you reduce the dimensionality of the data, you often discard some variation or detail in the original features. While this can simplify the problem, it may also result in a less accurate representation of the underlying data distribution.\n",
    "\n",
    "2. **Difficulty in Feature Interpretation**: In many cases, it becomes more challenging to interpret and explain the transformed features created by dimensionality reduction techniques. Understanding the meaning or importance of these new features can be less intuitive than interpreting the original features.\n",
    "\n",
    "3. **Parameter Tuning**: Some dimensionality reduction methods, such as Principal Component Analysis (PCA), require selecting hyperparameters, such as the number of components to keep. Determining the optimal number of components can be a non-trivial task and may require cross-validation or other techniques.\n",
    "\n",
    "4. **Algorithm Dependency**: The effectiveness of dimensionality reduction techniques can depend on the choice of algorithm and the nature of the data. What works well for one dataset may not work as effectively for another. Therefore, selecting the right technique requires experimentation and domain knowledge.\n",
    "\n",
    "5. **Computational Complexity**: Some dimensionality reduction techniques, especially those that involve matrix factorization or nonlinear transformations, can be computationally expensive, particularly for large datasets. This may limit their applicability in real-time or resource-constrained environments.\n",
    "\n",
    "6. **Assumption Violation**: Certain techniques, like PCA, assume that the data is linearly separable in the reduced space. If this assumption is not met, the transformed data may not capture the underlying structure effectively, leading to suboptimal results.\n",
    "\n",
    "7. **Loss of Discriminative Power**: In some cases, dimensionality reduction can inadvertently remove features that are crucial for the task at hand. This can lead to reduced discriminative power in the reduced feature space and negatively impact model performance.\n",
    "\n",
    "8. **Nonlinearity Handling**: Some real-world datasets exhibit nonlinear relationships that cannot be effectively captured by linear dimensionality reduction techniques. In such cases, more advanced nonlinear techniques like t-Distributed Stochastic Neighbor Embedding (t-SNE) may be required, but they can be computationally intensive.\n",
    "\n",
    "9. **Curse of Dimensionality for Clustering**: While dimensionality reduction can help mitigate the curse of dimensionality for some machine learning tasks, it can exacerbate the problem for clustering. Reduced-dimensional spaces may result in less separation between data points, making clustering more challenging.\n",
    "\n",
    "10. **Data Dependence**: The success of dimensionality reduction techniques can be highly dependent on the characteristics of the data. If the data is inherently low-dimensional or already well-structured, dimensionality reduction may not provide significant benefits and could even lead to a loss of valuable information.\n",
    "\n",
    "Despite these limitations, dimensionality reduction remains a valuable tool for simplifying high-dimensional data and improving the efficiency and effectiveness of machine learning algorithms. The choice of technique should be made carefully, taking into account the specific dataset and problem at hand, and potential trade-offs between dimensionality reduction and information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0d842-8126-4175-8b38-471cb22cbd4a",
   "metadata": {},
   "source": [
    "# Question.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0fd2b-a3b5-4299-8ef9-8f374be569f0",
   "metadata": {},
   "source": [
    "## How does the curse of dimensionality relate to overfitting and underfitting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b9d52-3396-4b10-a7b0-2006a794e711",
   "metadata": {},
   "source": [
    "The curse of dimensionality is closely related to the concepts of overfitting and underfitting in machine learning. Understanding this relationship is essential for building models that generalize well to unseen data. Here's how these concepts are interconnected:\n",
    "\n",
    "**1. Curse of Dimensionality and Overfitting:**\n",
    "\n",
    "- The curse of dimensionality often leads to overfitting. When you have a high-dimensional feature space with many features relative to the number of samples, machine learning models can easily become too complex and fit the noise in the training data rather than capturing the true underlying patterns.\n",
    "\n",
    "- High-dimensional spaces have more capacity for complex representations, which means that models can potentially memorize the training data instead of generalizing from it. This phenomenon results in models that perform extremely well on the training data but poorly on unseen data.\n",
    "\n",
    "- Overfitting occurs when a model is too complex and captures noise or fluctuations in the training data that are not representative of the underlying data distribution. In essence, the model becomes too tailored to the training data and lacks the ability to generalize to new data points.\n",
    "\n",
    "**2. Curse of Dimensionality and Underfitting:**\n",
    "\n",
    "- Paradoxically, the curse of dimensionality can also lead to underfitting, particularly when you have a limited amount of training data and a very high-dimensional feature space.\n",
    "\n",
    "- Underfitting occurs when a model is too simple to capture the underlying patterns in the data. In the presence of high dimensionality, the model may struggle to find meaningful relationships between features and the target variable. This results in a model that has a high training error and performs poorly both on the training and validation data.\n",
    "\n",
    "- High-dimensional spaces can exacerbate underfitting because the model may not have enough information to make accurate predictions, especially if the number of features is much larger than the number of training examples. The model may essentially ignore many of the features, resulting in poor performance.\n",
    "\n",
    "**3. Balancing Act:**\n",
    "\n",
    "- Balancing the complexity of the model with the dimensionality of the feature space is crucial. The goal is to find a model that is complex enough to capture the essential patterns in the data but not overly complex to the point of overfitting.\n",
    "\n",
    "- Feature selection, feature engineering, and dimensionality reduction techniques are often employed to strike this balance. By reducing the dimensionality of the data while retaining relevant information, these techniques can help prevent overfitting and underfitting.\n",
    "\n",
    "In summary, the curse of dimensionality can manifest as overfitting due to the increased complexity and sparsity of high-dimensional data. However, it can also lead to underfitting when the model struggles to find meaningful patterns in the data. Finding the right level of complexity and dimensionality reduction is essential for building models that generalize effectively to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2cb97d-ff92-4902-bb03-89ff61d12265",
   "metadata": {},
   "source": [
    "# Question.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf890ddf-6b34-4e19-a503-5191f95a265f",
   "metadata": {},
   "source": [
    "## How can one determine the optimal number of dimensions to reduce data to when using dimensionality reduction techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47fcdc-a432-4fce-aaa7-068222bbec68",
   "metadata": {},
   "source": [
    "Determining the optimal number of dimensions to reduce data to when using dimensionality reduction techniques is a critical but often challenging task. The choice of the number of dimensions can significantly impact the performance of your machine learning model. Here are some common strategies and techniques to help you make this decision:\n",
    "\n",
    "1. **Explained Variance**: For techniques like Principal Component Analysis (PCA), you can analyze the cumulative explained variance ratio as a function of the number of dimensions. Plotting this curve allows you to see how much variance in the data is retained as you increase the number of dimensions. A common approach is to choose the number of dimensions that captures a sufficiently high percentage (e.g., 95% or 99%) of the total variance.\n",
    "\n",
    "2. **Scree Plot**: In PCA, a scree plot shows the eigenvalues of the principal components in descending order. The point where the eigenvalues start to level off represents a natural choice for the number of dimensions to retain. This point often corresponds to an \"elbow\" in the plot.\n",
    "\n",
    "3. **Cross-Validation**: Use cross-validation to assess the performance of your machine learning model with different numbers of dimensions. For each dimensionality setting, split your data into training and validation sets, train the model, and evaluate its performance. Choose the number of dimensions that results in the best validation performance. Be cautious about overfitting during this process.\n",
    "\n",
    "4. **Out-of-Sample Evaluation**: If you have a separate test dataset that was not used during dimensionality reduction, you can evaluate your model's performance on this dataset for different numbers of dimensions. Choose the number of dimensions that maximizes test performance. This is a more robust approach than using cross-validation on the training data alone.\n",
    "\n",
    "5. **Domain Knowledge**: Consider domain-specific knowledge and the requirements of your machine learning task. Some applications may require a specific number of dimensions based on prior knowledge or constraints. For example, in image recognition, you might choose a fixed number of dimensions to match the architecture of a pre-trained model.\n",
    "\n",
    "6. **Feature Importance**: If you are using dimensionality reduction as a preprocessing step for a specific machine learning task, some algorithms provide feature importance scores (e.g., Random Forests or gradient boosting). These scores can help you identify which dimensions are most important for the task, guiding your choice of dimensions to retain.\n",
    "\n",
    "7. **Visualization**: In some cases, you can visually inspect the reduced-dimensional data to determine if it captures the essential patterns. Plotting the data in 2D or 3D can provide insights into whether the data is well-separated or retains relevant structure.\n",
    "\n",
    "8. **Trial and Error**: Experiment with different numbers of dimensions and observe the model's performance. Gradually increase or decrease the number of dimensions until you find a balance between model complexity and performance.\n",
    "\n",
    "9. **Autoencoders**: When using autoencoders for dimensionality reduction, you can experiment with different architectures and encoder/decoder layer sizes. Training autoencoders with various numbers of neurons in the bottleneck layer allows you to control the dimensionality of the encoded representation.\n",
    "\n",
    "10. **Information Criteria**: Techniques like Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) can be used to compare models with different numbers of dimensions. These criteria penalize model complexity and help in model selection.\n",
    "\n",
    "Ultimately, the choice of the optimal number of dimensions should be guided by a combination of empirical evaluation, domain knowledge, and the specific requirements of your machine learning task. It's often a trade-off between retaining sufficient information for accurate modeling and reducing dimensionality to improve computational efficiency and model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb1951-4e24-4e21-806b-7fb18fc9cf97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462dafd-7711-47f8-8933-f8a8cb8a5aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e25843-d1ea-4a19-9f83-60f1afa81d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30012d32-3714-4f34-9030-622c546a1080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
